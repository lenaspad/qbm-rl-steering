from collections import defaultdict
import numpy as np
import matplotlib.pyplot as plt

from qbm_rl_steering.environment.env_desc import TargetSteeringEnv



def make_epsilon_greedy_policy(Q, epsilon, n_actions):
    """
    Creates an epsilon-greedy policy based on a given Q-function and epsilon.

    Args:
        Q: A dictionary that maps from state -> action-values.
            Each value is a numpy array of length nA (see below)
        epsilon: The probability to select a random action . float between 0 and 1.
        n_actions: Number of actions in the environment.

    Returns:
        A function that takes the observation as an argument and returns
        the probabilities for each action in the form of a numpy array of length nA.

    """

    def policy_fn(observation):
        dim = Q[observation].shape
        greedy_action = np.argmax(Q[observation])
        probs = np.full(dim, epsilon / dim[0])
        probs[greedy_action] += 1 - epsilon
        return probs

    return policy_fn


def mc_control_epsilon_greedy(
        env, num_episodes, discount_factor=1.0, epsilon=0.1):
    """
    Monte Carlo Control using Epsilon-Greedy policies.
    Finds an optimal epsilon-greedy policy.

    Args:
        env: OpenAI gym environment.
        num_episodes: Number of episodes to sample.
        discount_factor: Gamma discount factor.
        epsilon: Chance the sample a random action. Float betwen 0 and 1.

    Returns:
        A tuple (Q, policy).
        Q is a dictionary mapping state to action values.
        policy is a function that takes an observation as an argument and
        returns action probabilities
    """

    # Keeps track of sum and count of returns for each state
    # to calculate an average. We could use an array to save all
    # returns (like in the book) but that's memory inefficient.
    returns_sum = defaultdict(float)
    returns_count = defaultdict(float)

    # The final action-value function.
    # A nested dictionary that maps state to (action to action-value).
    Q = defaultdict(lambda: np.zeros(env.action_space.n))

    # A nested dictionary that maps state to (action to number of times
    # state-action pair was encountered).
    N = defaultdict(lambda: np.zeros(env.action_space.n))

    iterations = 0

    # policy improvement: this function holds a reference to the Q_values
    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)
    while iterations < num_episodes:
        done = False
        episode = []
        # visited_states = {}
        visited_states = set()
        s = env.reset()
        s = tuple(s)
        while not done:
            # choose an action based on a probability dist generated by
            # policy(), epsilon/ |A(s)| chance of random action
            action = np.random.choice(range(env.action_space.n), p=policy(s))
            new_s, r, done, _ = env.step(action)
            new_s = tuple(new_s)
            episode.append((s, action, r))
        for state, action, reward in episode:
            # first-visit monte carlo update
            if state not in visited_states:
                N[state][action] += 1
                # incremental update of Q value is more memory efficient than
                # simply keeping a record of all rewards
                # and averaging after every new reward
                Q[state][action] += (discount_factor * (1. / N[state][action]) *
                                     (reward - Q[state][action]))
                visited_states.add(state)

        iterations += 1

    return Q, policy











env = TargetSteeringEnv()
Q, policy = mc_control_epsilon_greedy(
    env, num_episodes=10000, discount_factor=0.8, epsilon=0.5)

s_floats = []
q_values = []
for k, v in Q.items():
    s_floats.append(env.make_binary_state_float(np.array(k)))
    q_values.append(v)

s_floats = np.array(s_floats)
q_values = np.array(q_values)

idx_srt = np.argsort(s_floats)
s_floats = s_floats[idx_srt]
q_values = q_values[idx_srt]
for a in range(env.action_space.n):
    plt.plot(s_floats, q_values[:, a], label=f'Action {a}')
plt.legend()
plt.show()


